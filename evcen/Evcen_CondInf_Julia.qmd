---
title: "Ebru Evcen: Conditional Inference"
subtitle: "RePsychLing in SMLP2023"
author: "Reinhold Kliegl"
date: "2023-09-08"
format:
  html:
    embed-resources: true
    toc: true
    toc-depth: 2
    code-fold: false
    number-sections: true
    fig-width: 8
    fig-height: 6
editor_options:
  chunk_output_type: console
jupyter: julia-1.9
---

# Description


# Setup

## Packages

```{julia}
#| label: packages

using Arrow
using CairoMakie
using DataFrames
using MixedModels
using MixedModelsMakie
using RegressionFormulae

CairoMakie.activate!(; type="svg");
```

## Code book

+ `Subj`, `Item`: Grouping variables (blocking factors)
+ `Cat`, `Resp`:  within-Subj and within-Item fixed factors 
+ `Type`:  within-Subj and between-Item factor
  
```{julia}
#| label: data
dat = DataFrame(Arrow.Table("data/Evcen_CondInf.arrow"));
describe(dat)
```

```{julia}
levels(dat.Type)
```

```{julia}
levels(dat.Cat)
```

```{julia}
levels(dat.Resp)
```

# Contrasts

We have  the random factors `Subj` and `Item` and declare them as a grouping variables.

```{julia}
contrasts = Dict(:Subj => Grouping(),
                 :Item => Grouping(),
                 :Type => EffectsCoding(),
                 :Cat => EffectsCoding(),
                 :Resp => EffectsCoding())
```

Grouping variables are automatically detected by the program. So this line is not really needed, but good to know.

# Model selection

## Only varying intercept LMM

A quick way to check highest order interactions. 
```{julia}
#| label: m_ovi1

m_ovi1 = let
    form = @formula(log(rt) ~ 1 + (Type + Cat + Resp)^1 + 
                              (1 | Subj) + 
                              (1 | Item))
    fit(MixedModel, form, dat; contrasts)
end
```

```{julia}
#| label: m_ovi2

m_ovi2 = let
    form = @formula(log(rt) ~ 1 + (Type + Cat + Resp)^2 + 
                             (1 | Subj) + 
                             (1 | Item))
    fit(MixedModel, form, dat; contrasts)
end
```

```{julia}
#| label: m_ovi3

m_ovi3 = let
    form = @formula(log(rt) ~ 1 + (Type + Cat + Resp)^3 + 
                             (1 | Subj) + 
                             (1 | Item))
    fit(MixedModel, form, dat; contrasts)
end
```

```{julia}
#| label: lrt1

MixedModels.likelihoodratiotest(m_ovi1, m_ovi2, m_ovi3)
```

Most likely don't need 3-factor interaction. 

## Complex LMM

 We start with a complex LMM; it is not the maximal LMM because we do not include interaction terms in the RES.

```{julia}
#| label: m_cpx

m_cpx = let
    form = @formula(log(rt) ~ 1 + (Type + Cat + Resp)^2 + 
                             (1 + Type + Cat + Resp | Subj) + 
                             (1 + Cat + Resp | Item))
    fit(MixedModel, form, dat; contrasts)
end
```

```{julia}
issingular(m_cpx) # not ok, overparameterized
```

```{julia}
VarCorr(m_cpx)
```

This model is overparameterized.

## Zero-correlation parameter LMM

```{julia}
#| label: m_zcp

m_zcp = let
    form = @formula(log(rt) ~ 1 + (Type + Cat + Resp)^2 + 
                     zerocorr(1 + Type + Cat + Resp | Subj) +
                     zerocorr(1 + Cat + Resp | Item))
    fit(MixedModel, form, dat; contrasts)
  end
```

```{julia}
issingular(m_zcp) # not ok, overparameterized
```

```{julia}
VarCorr(m_zcp)
```

LMM `m_zcp` is still overparameterized. 
No evidence for reliable VCs for Subj-related VCs for `Type` and `Cat`.

# Parsimonious LMM (1)

We remove zero VCs from RES.

```{julia}
#| label: m_prsm1

m_prm1 = let
    form = @formula(log(rt) ~ 1 + (Type + Cat + Resp)^2 + 
                    zerocorr(1 + Resp | Subj) + 
                    zerocorr(1 + Cat + Resp | Item))
    fit(MixedModel, form, dat; contrasts)
end
```

```{julia}
issingular(m_prm1) #  ok
```

```{julia}
VarCorr(m_prm1)
```

```{julia}
let mods = [m_ovi2, m_prm1, m_zcp, m_cpx];
    DataFrame(;
              model=[:m_ovi, :m_prm1, :m_zcp, :m_cpx],
              pars=dof.(mods),
              geomdof=round.(Int, (sum âˆ˜ leverage).(mods)),
              AIC=round.(Int, aic.(mods)),
              AICc=round.(Int, aicc.(mods)),
              BIC=round.(Int, bic.(mods)))
end
```

AIC and BIC: select m_prm1.

# Parsimonious LMM (2)

We add the CPs back in. 

```{julia}
#| label: m_prm2

m_prm2 = let
    form = @formula(log(rt) ~ 1 + (Type + Cat + Resp)^2 + 
                             (1 + Resp | Subj) + 
                             (1 + Cat + Resp | Item))
    fit(MixedModel, form, dat; contrasts)
end
```

```{julia}
issingular(m_prm2) #  ok
```

```{julia}
VarCorr(m_prm2)
```

```{julia}
#| label: compare2
MixedModels.likelihoodratiotest(m_prm1, m_prm2)
```

No improvement in goodness of fit. We stay with `m_prm1.`

# Figures

## Caterpillar plots

```{julia}
#| fig-cap1: Prediction intervals on Subj random effects for model m_prm1
#| label: fig-cm_prm1_Subj

reinfo = ranefinfo(m_prm1)
caterpillar!(Figure(; resolution=(800, 1200)), reinfo[:Subj]; orderby=1)
```


```{julia}
#| fig-cap1: Prediction intervals on Item random effects for model m_prm1
#| label: fig-cm_prm1_Item

caterpillar!(Figure(), reinfo[:Item]; orderby=1)
```

## Shrinkage plot

```{julia}
#| code-fold: true
#| label: fig-shrinkage_Subj
#|
#| fig-cap: Shrinkage plots of the Subj random effects in model m_prsm1
shrinkageplot(m_prm1, :Subj)
```
```{julia}
#| code-fold: true
#| label: fig-shrinkage_Item
#|
#| fig-cap: Shrinkage plots of the Item random effects in model m_prsm1
shrinkageplot!(Figure(; resolution=(1000, 1200)), m_prm1, :Item)
```


# Appendix
```{julia}
versioninfo()
```
