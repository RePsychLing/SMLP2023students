---
title: "SMLP 2023 Questions"
author: "Tatiana Pashkova"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    number_sections: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Background 

## Main goal

The goal of this analysis is to compare speech rates (in syllables/second) of four groups of bilingual English speakers to one group of monolingual English speakers. The bilinguals are heritage speakers of German, Greek, Russian, and Turkish who grew up in the US and speak English as their dominant language. They learnt the heritage languages in the family, mostly from one or both parents. The monolinguals are speakers who grew up in the US as well and who learnt English as their only L1 in the family (even though most of them learnt various foreign languages at school).

By modelling the speech rates we would like to show that bilingual and monolingual speakers in our sample are not different from each other in how fast they speak, and hence are likely to have similar proficiency in English.

## Variables

**Name** - the file names of spoken files in which the speech rates were calculated                    
Categorical variable with 574 levels

**Speaker** - the speakers who produced the spoken files                       
Categorical variable with 287 levels - each speaker produced two spoken files   

**Speaker group**                
Categorical variable with 5 levels: German HS, Greek HS, Russian HS, Turkish HS, English MS       
HS stands for "heritage speaker", MS - for "monolingual speaker"    

**Speech rate**                    
Ratio-scaled numerical variable, syllables per second

## Main questions

**(1) When fitting a linear mixed model, should we go with REML = FALSE or TRUE?**                  
Winter (2019, p. 254) says we should do REML = FALSE but I never really understood why so I can't make an informed choice. In the modelling section, you will see that I used both and the results are very similar.

RK: Yes, with many observations there will not be much of a difference. We will cover this at SMLP2023.

**(2) After fitting a linear mixed model, what is the exact list of assumptions that I should check and what is the best way to check them?**                                
In the modelling section, you will see that I gathered the assumptions from different sources, but I am not sure that I'm not missing anything. In addition, you will see that I'm not sure how to check some of the assumptions.

RK:  There are some degrees of freedom in interpreting diagnostic checks. We will cover this at SMLP2023.

**(3) Do I need to check for outliers and influential observations? If so, what is the best way and what do I do with the identified observations?**

RK: Also a good question and will also be covered at SMLP2023. Make sure we follow up on my promises :)

# Data examination

## Data pre-processing

Loading the packages

```{r, message = FALSE}
library(readxl)
library(tidyverse)
library(ggridges)
library(lme4)
library(lmerTest)
```

Importing the data frame

```{r, message=FALSE}
read_excel("data/speech_rate_corpus_speakers_final_data_frame_aug_2023.xlsx") -> syll
```

Making all file names capital to avoid the problems with capitalization (Usbi25FT, USbi25FT)

```{r}
syll$name = toupper(syll$name)
```


Adding new columns "speaker" and "speaker_group" by taking substrings of characters from file name

```{r}
syll %>% 
  mutate(speaker = (substr(name, 1, 8))) %>% 
  mutate(speaker_group = (substr(name, 8,8)))  -> syll

syll$speaker_group <- recode_factor(syll$speaker_group, D = "German_HS", 
                                                        E = "English_MS",
                                                        G = "Greek_HS",
                                                        R = "Russian_HS",
                                                        `T` = "Turkish_HS")
```
Giving each observation an individual number

```{r}
syll %>% 
  mutate(num=as.numeric(rownames(syll))) -> syll
```


Renaming the speech rate column

```{r}
syll %>% 
  rename(speech_rate = `speechrate(nsyll/dur)`) -> syll

```

Removing the columns that are not used in the analysis

```{r}
syll %>% 
  select(num, name, speaker, speaker_group, speech_rate) -> syll
```

Relevelling the column "speaker_group" so that English MSs are the reference level and come first in the data exploration above

```{r}
syll$speaker_group <- relevel(syll$speaker_group, ref = "English_MS")
```


## Exploratory data analysis

### Univariate

Histogram and density plot of **the speech rate**, irrespective of speaker group

```{r}
# Create a histogram
hist(syll$speech_rate, freq = FALSE, breaks = 25, main = "Histogram and density")

# Calculate density
dsr <- density(syll$speech_rate)

# Add density
lines(dsr, lwd = 2, col = "steelblue")

# Add the data-points in the X-axis
rug(syll$speech_rate)
```
Normality test for **the speech rate** (as used by Gries, 2021, p. 177) - the values seem normally-distributed

```{r}
nortest::lillie.test(syll$speech_rate)
```

Counting files and speakers in each **speaker group** 

```{r}
syll %>% 
  group_by(speaker_group) %>% 
  summarize(n_files = n(), n_speakers = n_files/2)
```

### Bivariate

Dot plot of speech rate by speaker group, one dot - one spoken file
 
```{r}
syll %>% 
  ggplot(aes(x = speaker_group, y = jitter(speech_rate))) +
  geom_point(alpha = 0.5)
```





Box plot by speaker group

```{r}
syll %>% 
  ggplot(aes(x = speaker_group, y = speech_rate, fill = speaker_group)) + 
  geom_boxplot()
```

Density plot by speaker group - in each speaker group, speech rates do not seem as normally distributed as all speech rates together

```{r}
syll %>% 
  ggplot(aes(x = speech_rate, y = speaker_group)) +
  geom_density_ridges(alpha = 0.5, fill = "steelblue") +
  theme_bw() +
  xlab("Speech_rate") +
  ylab("Speakergroup")
```
However, the normality tests say that in each speaker group the speech rates are still normally distributed...

```{r}
nortest::lillie.test(syll$speech_rate[syll$speaker_group == "English_MS"])
nortest::lillie.test(syll$speech_rate[syll$speaker_group == "German_HS"])
nortest::lillie.test(syll$speech_rate[syll$speaker_group == "Greek_HS"])
nortest::lillie.test(syll$speech_rate[syll$speaker_group == "Russian_HS"])
nortest::lillie.test(syll$speech_rate[syll$speaker_group == "Turkish_HS"])
```

Summary statistics by speaker group

```{r}
syll%>% 
  group_by(speaker_group) %>% 
  summarise(mean_speech_rate = mean(speech_rate), 
            sd = sd(speech_rate), 
            min = min(speech_rate), 
            max = max(speech_rate))
```


# Fitting linear mixed models

I am fitting two models, one with REML = FALSE and one with REML = TRUE. This refers back to my first main question as to how to make the right choice between these two options.

I am using the default treatment contrast coding because I would like to compare every heritage speaker group to English monolinguals. English monolinguals are already set as a reference level.

## Model 1

REML = FALSE, as recommended by Winter (2019, p. 254).

RK: Also recommended by Bates. The default recommendation for lme4 is also to use `control=lmerControl(calc.derivs=FALSE)`

```{r}
model1 <- lmer(speech_rate ~ speaker_group + (1|speaker), data=syll, REML = FALSE)
summary(model1)
```

## Model 2

REML = TRUE, for comparison with Model 1

```{r}
model2 <- lmer(speech_rate ~ speaker_group + (1|speaker), data=syll)
summary(model2)
```

We see that the results of Model 1 and Model 2 are nearly identical. So, should I care about the REML argument? If so, when?


# Checking model assumptions

In this section, I am going to list all the assumptions that I have seen in Gries (2021) and Levshina (2015). I will also show my attempts to check these assumptions and write down my questions.

I will work with Model 1. 


## Independent observations 

Levshina (2015, p.155) says that the observations need to be independent.                   

However, we have a mixed effects model, so we are fine.                        


## DV is at least on interval scale

Levshina (2015, p.156) says that we should either have an interval-scaled data (zero does not mean an absence of a values, e.g. 0 degrees Celsius) or a ratio-scaled data (zero means an absence of the variable, e.g. no weight, speed). We should not be fitting a linear model on ordinal data, such as ratings.

We have a ratio-scaled variable: zero means an absence of speech rate; moreover 1 vs 2 syllables per secon is the same difference as 2 vs 3 syllables per second so our data is not ordinal.         



## Linear relationship between DV and IV

Levshina (2015, p.156) mentions that DV and IV should have a linear relationship.                   

I have several questions about this assumption:

(1) Isn't it covered by the assumption of the normality of the residuals?                            
(2) Or isn't it covered by the homoscedasticity of the residuals?                
(3) If we are modelling a continuous response on a categorical predictor, and we only compare 2 levels at a time, how can we have a non-linear relationship? I guess you can always draw a line between 2 means of 2 levels of the IV, and in that sense the relationship is always linear?? I can't imagine any curved shape...

On the same page, Levshina recommends using a partial-residual plot to detect non-linearity. However, the function `crPlot` doesn't seem to work with a mixed model.

```{r}
car::crPlot(model1, var = "speaker_group")
```
Hence, another question:                     
(4) Do I need to worry about looking for another function that will work with a mixed model? If so, do you have any recommendations?                      


## Homoscedasticity of residuals along DV 

The residuals should vary constantly against the DV (Levshina, 2015, p. 157; Gries, 2021, p. 424).            
For example, it shouldn't happen that for the lower fitted values of the DV the residuals are small, and for the upper fitted values of of the DV the residuals are large.             

Plotting residuals against fitted values (Gries, 2021 p. 424)

```{r}
plot(model1, col="#00000020", # plot the residuals of this model
     type=c("p", "smooth"),   # w/ points & a smoother
     pch=16, id=0.001)       # identify outliers w/ this sign level
#overall, seems like there is some correlation between residuals and predicted values - dunno what to do
```
Same plot, from https://www.statology.org/residual-plot-r/ and Winter (2019, p. 111)

```{r}
plot(fitted(model1), residuals(model1))
abline(0,0)
```

It seems that residuals are more negative when fitted speech rate is low, and more positive then the fitted speech rate is high. But is the pattern skewed enough? At which point do we say that residuals are heteroscedastic?


Levshina (2015, p. 157) suggests a non-constant variance test, but it doesn't work:

```{r}
car::ncvTest(model1) 
```

Is there any other test one can use? Should one use tests at all?


## Homoscedasticity of residuals along IV 

Residuals should also vary constantly against the IV (Levshina, 2015, p. 158; Gries, 2021, p. 424). So there shouldn't be a situation where for lower values of the IV, the residuals are small, and for upper values of the IV, the residuals are large. In our case, I assume it means that we shouldn't have large residuals in one speaker group and small residuals in another one.

Visual examination of residuals against levels of the IV, suggested by Gries (2021, p. 424). I can't see big differences between the speaker groups:

```{r}
stripchart(                                     # plot a stripchart
  residuals(model1) ~ syll$speaker_group,       # residuals ~ speaker group
  xlab="speaker_group",                         # x-axis label
  method="jitter", vertical=TRUE,               # jitter points & orient plot vertically
  pch=16, col="#00000020"); grid()              # filled circles, transparent grey, add grid

```
Levshina (2015, p. 158) suggests to use another non-constant variance test, but it doesn't work either:

```{r}
car::ncvTest(model1, ~ speaker_group) 
```

How do I know if I have something to worry about regarding the homoscedasticity of residuals along the IV? Do the speaker group clouds in the plot look similar enough for me to be sure there is no heteroscedasticity?


## No autocorrelation of residuals

Levshina (2015, p. 161): Autocorrelation of residuals can happen in time series (e.g. temperatures), this is when a residual for one data point is correlated to the residual for the previous data point. Here I don't think it can happen, but I went along with the test Levshina does.

The firs attempt, exactly like in the book, doesn't work. The second attempt gives me some value but I don't know how to interpret it.

```{r}
car::durbinWatsonTest(model1)

car::durbinWatsonTest(residuals(model1)) 
```

If I happen to care about the autocorrelation of residuals in a future study, how do I check for it?

## Normality of residuals

Levshina (2015, p. 162) says that residuals should be normally distributed with the mean of 0.


Histogram of residuals - looks quite normal

```{r}
hist(residuals(model1), main="", breaks = 20)
```

Density plot of residuals - the tip isn't smooth but the whole shape looks normal enough

```{r}
density(residuals(model1)) -> density_model1
plot(density_model1)
```
Mean of residuals is very close to zero

```{r}
mean(residuals(model1))
```

Shapiro-Wilk normality test as suggested by Levshina (2015, p. 162) - the residuals are not normal! Despite the decent-looking histogram and density plot.

```{r}
shapiro.test(residuals(model1))
```
Kolmogorov-Smirnov test for normality, which is used on many occassions by Gries but not on the residuals. I decided to try it anyways - and this test says that the residuals are normal!

```{r}
nortest::lillie.test(residuals(model1))
```
Levshina also uses a Q-Q plot, but the exact code that she gives on p. 162 isn't plotting the residuals against theoretical quartiles, it plots them against fitted values.

```{r}
plot(model1, which = 2) 
```

So I tried using a Q-Q plot differently, based on Winter (2019, p. 111). The dots are mostly on the line, which means the residuals should be normal?

```{r}
qqnorm(residuals(model1))
qqline(residuals(model1))

```


Here my question is: Do you recommend using tests to figure out if residuals are normal? If so, which test to use and why do different tests show different results? If not, what visual means do you recommend using?


## Normality of random intercept adjustments

Gries (2021, p. 431): "Varying intercepts for the speakers <...> are supposed to be normally distributed around 0" (even though non-normally distributed ones introduce only a modest bias, says the footnote).

Plot 1

```{r,fig.height = 25, fig.width=8}
lattice::dotplot(ranef(model1, condVar=TRUE))
```


Plot 2

```{r,fig.height = 25, fig.width=8}
plot(model1, speaker ~resid(.))
```

I think both plots look like the assumption is met, however, I'm not 100% sure. What should I pay attention to when trying to decide if the random intercepts are normally-distributed about 0? Is there a way you would recommend?

Relatedly, what would you do if you had non-normal random intercepts? For example, there would be two clusters - some speakers speak really fast and some really slow. Would you split your data set and fir different models? This is obviously not the case here, but I had such a situation with my items in another study.

# Power transformations if violated assumptions

Overall, based on the previous section, I have doubts about the homoscedasticity and normality of residuals.       

If I decided my residuals are heteroscedastic and non-normal (maybe for this case, maybe for another case), my question is how to figure out what to do with this problem.

Levshina (2015, p. 158) recommends the Box-Cox test to decide which power transformation one should apply to the DV in order to fix the model. However, the boxCox function does not seem to work.

```{r}
car::boxCox(model1, lambda = seq(-3, 3, 1/10))

```
Other ways to deal with heteroscedastic and non-normal residuals that I know would be to fit a non-linear model, for example, a GAMM, or to include a polynomial term into the linear model. However, these methods basically fit a curvy line thought the data, and again, I don't understand how we can fit a curvy line if we compare a numerical DV between two levels of IV.



# Influential observations and outliers

Do I need to check for them, is this a part of a model fitting flow that you would recommend?


Gries (2021, p. 378) says the following:

"There are some useful statistics that can be computed very easily to get this kind of information from a model object.        
Three conceptually fairly straightforward statistics one might explore for this are the following:               
– dfbeta, which answer the question “how much does deleting the i-th observation change the regression              coefficient?”; this means, you get one dfbeta-value for each data point and each regression coefficient;             
– dffits, which answer the question “how much does deleting the i-th observation change the prediction of the i-th data point?”; this means, you get one dffits-value for each data point;              
– Cook’s D with cooks.distance, which answer the question "how much does deleting the i-th observation change all predicted values? This means, you get one D-value for each data point"                    

The first two functions don't seem to work:

```{r}
dfbeta(model1)
dffits(model1)
```

This either:

```{r}
influence.measures(model1, infl = influence(model1))
```

This works, here are the 10 observations with the highest Cook's distance:

```{r}
cooks.distance(model1) -> cooksd
head(sort(cooksd, decreasing = TRUE), 10)
```

Levshina (2015, p. 153) uses the following function:

```{r}

car::influencePlot(model2, id.method = "identify") 

```

Both methods (Cook's distance for every observation and the combination of Cook's distance, hat-values and studentized residuals) seem to point out observations 565 and 566. However, observations 200 and 58 could be seen as outliers too... So do I have to remove any observations, and if so, how do I decide which ones?


If I do remove 565 and 566, my model changes only a tiny bit, so it doesn't really help my decision...

```{r}
#Model 3
model3 <- lmer(speech_rate ~ speaker_group +(1|speaker), data=syll[-c(565, 566),  ], REML = FALSE)
summary(model3)

# Residuals against fitted values
plot(model3, col="#00000020", # plot the residuals of this model
     type=c("p", "smooth"),   # w/ points & a smoother
     pch=16, id=0.001)    

#Residuals against levels of IV
stripchart(                                     # plot a stripchart
  residuals(model3) ~ syll$speaker_group[-c(565, 566)],       # residuals ~ speaker group
  xlab="speaker_group",                         # x-axis label
  method="jitter", vertical=TRUE,               # jitter points & orient plot vertically
  pch=16, col="#00000020"); grid() 

#Residual density plot
density(residuals(model3)) -> density_model3
plot(density_model3)
```


# RK comments

1. We will cover your questions about residual and other diagnostics at SMLP2023. In advance, I must say that I usually only look at a handful of the ones you mention and judgment on whether to take action or nor is mostly based on visual inspection diagnostic plots and experience with data from the fields I have been working in. (I am not saying that it is not useful to look for inferential statistics; only that I never was asked for them by reviewers.)

+ Box-Cox to check normality of residuals (not observations);  a suitable transformation of the data usually takes care of the problems you later find in the other diagnostics mentioned below.
+ qq-plot to check assumption of residual normality
+ residual over fitted to check heteroskedasticity; also to check whether there are outlier observations that cause a large top-bottom asymmetry. If they are found I remove the observations and refit the LMM (see example below),
+ normality of conditional modes (BLUPs): caterpillar plots. They also have heuristic value about whether there are clusters of units that you may want to take care of in the fiixed-effect part of the lmm; they may also guide the next study. 
+ autocorrelation makes only sense if you have long timeseries of observations from the units (subjects). In your case, you only have two observations, so it does not come into play. 

There is a nice suite in the easystats suite of packages. For lm and lme4 models I usually use, `performance::check_model(model)` (see below).

2. We also will cover your questions about linear and nonlinear functions. In advance, a two-level factor can be thought of as a linear trend; nothing else. A three-level factor can be thought of as a quadratic trend if the levels are ordered and equidistant, nothing else; and so on. 

3. The main short-coming of your example is that you only have two observations per subject. That does not give you much room for stable estimates of variance components and correlation parameters. You only estimated the VC for the intercept. So this is ok. 
    
4. What do the two measures/subject represent? Is this an experimental manipulation or a simple test-retest situation? In the latter case, and especially if the correlation between the two measures within the groups is very high (and you are not interested in the difference), you may just as well take the average and run a multiple regression. There would be no need for an LMM. 

5. I attach a slightly modified version of your script to show how you can write a file for input to Julia and a Julia-based qmd script that illustrates how you specify mixed models there and also a few diagnostic plot options you have there. 


```{r}
library(easystats)
check_model(model1, check=c("pp_check", "linearity",  "homogeneity",  "normality", "reqq", "qq") )
```

One can see the two outliers you also identified.

```{r}
check_model(model3, check=c("pp_check", "linearity",  "homogeneity",  "normality", "reqq", "qq") )
```

There is still some asymmetry with two large negative residuals in the top right panel. I probably would remove them, too. 

Output for input to Julia, I also change names to comply with my style conventions (i.e., factors start with capital letters, continuous variables with small letters; you don't have to adopt my style!). I also prefer short variable names.

```{r}
dat <- 
  syll |> 
  rename(Spkr = speaker, Group = speaker_group, rate = speech_rate) |> 
  select(Spkr, Group, rate)

# check the correlation between the two measures
dat |> pivot_wider(id.cols=Subj, )

library(arrow)
write_feather(dat, "data/Pashkova_SpeechRate.arrow")
```






